{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40fe6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dac51dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = torch.squeeze(train_dataset[0][0])\n",
    "# fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "# ax.imshow(img, cmap=plt.cm.gray)\n",
    "%config Completer.use_jedi = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3873556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloader \n",
    "batch_size = 20\n",
    "train_loader = DataLoader(\n",
    "                dataset= train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False) \n",
    "test_loader = DataLoader(\n",
    "                dataset = test_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eac1692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting dtype of the tensors\n",
    "class NeuralNet():\n",
    "    def __init__(self, LR, batch_size):\n",
    "        self.layer1 = np.random.uniform(-0.5, 0.5, (784, 64)).astype(np.float64)\n",
    "        self.layer2 = np.random.uniform(-0.5, 0.5, (64, 10)).astype(np.float64)\n",
    "        self.LR = LR\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        denominators = torch.sum(np.exp(x), 1)\n",
    "        for i in range(len(x)):\n",
    "            x[i] = np.divide(x[i], denominators[i])\n",
    "        return x\n",
    "    \n",
    "    def non_linearity(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.squeeze(x).view(-1, 784)\n",
    "        self.input = x.clone()\n",
    "\n",
    "        x = x @ self.layer1\n",
    "        x = self.non_linearity(x)\n",
    "        \n",
    "        self.z1 = x.clone()\n",
    "        x = x @ self.layer2\n",
    "        \n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        self.softmax_result = x.clone().float().T\n",
    "        return x\n",
    "    \n",
    "    def der_sigmoid(self, x):\n",
    "        return (x @ (1 - x).T).float()\n",
    "    \n",
    "    def der_mse(self):\n",
    "        d_loss_wrt_preds = (-2/torch.numel(outputs)) * (gt - outputs)\n",
    "        return d_loss_wrt_preds\n",
    "    \n",
    "    def backpass(self, loss, outputs, gt):\n",
    "        d_loss_wrt_preds =  (-2/torch.numel(outputs)) * (gt - outputs)\n",
    "\n",
    "        delta_w1 = self.input.T @ self.der_sigmoid(self.z1) @ d_loss_wrt_preds.float() @ self.der_softmax() @ self.layer2.T \n",
    "        delta_w2 = self.z1.T.float() @ d_loss_wrt_preds.float() @ self.der_softmax().float() \n",
    "\n",
    "        self.layer1 = torch.Tensor(self.layer1).float() - self.LR * delta_w1.float()\n",
    "        self.layer2 = torch.Tensor(self.layer2) - self.LR * delta_w2\n",
    "\n",
    "    def der_softmax(self):\n",
    "        softmax_d = torch.empty(20, 10, 10)\n",
    "        \n",
    "        for elem in range(self.batch_size):\n",
    "            SM = self.softmax_result.T[elem].reshape((-1,1))\n",
    "            jac = np.diagflat(self.softmax_result.T[elem]) - np.dot(SM, SM.T)\n",
    "            softmax_d[elem] = jac\n",
    "        return torch.sum(softmax_d, 0).float()\n",
    "    \n",
    "    def one_hot_encoding(self, y):\n",
    "        return F.one_hot(y, num_classes=10)\n",
    "    \n",
    "    def mean_sq_error(self, true, prediction):\n",
    "        error = np.square(np.subtract(true, prediction)).mean()\n",
    "        return error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4c2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                       | 41/3000 [00:00<00:14, 201.66it/s]/tmp/ipykernel_5823/197993562.py:16: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-x))\n",
      "100%|██████████████████████████████████████| 3000/3000 [00:13<00:00, 222.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for EPOCH=0 is 346.78260478442314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3000/3000 [00:13<00:00, 225.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for EPOCH=1 is 664.7084445126425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████▌                                | 501/3000 [00:02<00:11, 218.78it/s]"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "def training(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    for epoch in range(EPOCHS):\n",
    "        for x,y in tqdm(train_loader):\n",
    "            outputs = net.forward(x)\n",
    "            gt = net.one_hot_encoding(y)\n",
    "            batch_loss = net.mean_sq_error(gt, outputs)\n",
    "#             print(batch_loss)\n",
    "            net.backpass(batch_loss, outputs, gt)\n",
    "#             break\n",
    "#         break\n",
    "            total_loss = total_loss + batch_loss\n",
    "        print(f\"Loss for EPOCH={epoch} is {total_loss}\")\n",
    "\n",
    "\n",
    "\n",
    "LR = 0.001\n",
    "net = NeuralNet(LR, batch_size)\n",
    "training(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af80bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "def testing(test_loader, net):\n",
    "    acc = 0\n",
    "    total_correct = 0\n",
    "    for x, y in test_loader:\n",
    "        output = net.forward(x)\n",
    "        prediction = torch.argmax(output)\n",
    "        correct_predictions_batch = torch.count_nonzero(torch.eq(prediction, y))\n",
    "        total_correct = total_correct + correct_predictions_batch\n",
    "    return total_correct / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "196e8f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5823/197993562.py:16: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0920)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing(test_loader, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffce424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
