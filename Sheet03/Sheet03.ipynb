{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "752f660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d42d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Epochs = 20\n",
    "classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2704946",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get model from asignment 2 and check for acc and other metrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8776a2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "t = transforms.Compose([transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                       ])\n",
    "target_t = lambda label: F.one_hot(torch.tensor(label), num_classes=10)\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform = t, target_transform=target_t, download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform = t, target_transform=target_t, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f91474bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39cf1cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"airplane\",\n",
    "\"automobile\",\n",
    "\"bird\",\n",
    "\"cat\",\n",
    "\"deer\",\n",
    "\"dog\",\n",
    "\"frog\", \n",
    "\"horse\",\n",
    "\"ship\",\n",
    "\"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcda5555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = next(iter(test_loader))[0][1].permute(1,2,0)\n",
    "label = next(iter(test_loader))[1][1]\n",
    "def visual(img):\n",
    "    img = img / 2 + 0.5\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(15,5)\n",
    "\n",
    "    img = img.numpy()\n",
    "    im = ax.imshow(img, cmap=plt.cm.gray)\n",
    "    fig.colorbar(im)\n",
    "    ax.set_title(\"Image\")\n",
    "\n",
    "visual(img)\n",
    "print(label_names[torch.argmax(label)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4ce68860",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the CNN architecture to evaluate CIFAR-10 dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout: bool, name: str):\n",
    "        super().__init__()\n",
    "        \n",
    "#         Input: (3, 32, 32)\n",
    "        self.name = name\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=0)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         After Conv1 and maxpool: (6, 14, 14) \n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "\n",
    "#         After Conv2 and maxpool: (16, 5, 5)\n",
    "        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
    "        self.dropout = dropout\n",
    "        if self.dropout:\n",
    "            self.in_dropout = nn.Dropout2d(p=0.2)\n",
    "            self.cnn_dropout = nn.Dropout(p=0.5)\n",
    "            self.lin_dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.dropout:\n",
    "            x = self.in_dropout(x)\n",
    "            x = self.maxpool(F.relu(self.cnn_dropout(self.conv1(x))))\n",
    "            x = self.maxpool(F.relu(self.cnn_dropout(self.conv2(x))))\n",
    "            x = torch.flatten(x, start_dim=1)\n",
    "            x = F.relu(self.lin_dropout(self.fc1(x)))\n",
    "            x = F.relu(self.lin_dropout(self.fc2(x)))\n",
    "            x = self.fc3(x)\n",
    "        else:\n",
    "            x = self.maxpool(F.relu(self.conv1(x)))\n",
    "            x = self.maxpool(F.relu(self.conv2(x)))\n",
    "            x = torch.flatten(x, start_dim=1)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e3149d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_dropout = Network(dropout=True, name=\"with_dropout\")\n",
    "model_without_dropout = Network(dropout=False, name=\"without_dropout\")\n",
    "# summary(model_without_dropout, (3,32,32)) \n",
    "#thowing error check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f3e3c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training \n",
    "LR = 3e-4\n",
    "EVAL_FREQ = 1\n",
    "SAVE_FREQ = 10\n",
    "stats = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"valid_loss\": [],\n",
    "    \"accuracy\": []\n",
    "}\n",
    "init_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8682f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(params=model_without_dropout.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "186cd45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_model(model, optimizer, epoch):\n",
    "    if not os.path.exists(\"models\"):\n",
    "        os.makedirs(\"models\")\n",
    "    save_path = f\"models/{model.name}_{epoch}.pth\"\n",
    "    torch.save({\n",
    "        'epoch' : epoch,\n",
    "        'model_state_dict' : model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "#         'stats': stats\n",
    "    }, save_path)\n",
    "    \n",
    "def training(model, EPOCHS=100):\n",
    "#     Training time per epoch calculate and display\n",
    "    model = model.to(device)\n",
    "    loss_list = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss = []\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\n",
    "        for idx, (data, target) in pbar:\n",
    "            data, target = data.to(device), target.type(torch.FloatTensor).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss =+ loss.item()\n",
    "            epoch_loss.append(loss.item())\n",
    "            if epoch % 5 == 0:\n",
    "                saving_model(model, optimizer, epoch)\n",
    "            \n",
    "            if idx % 5 == 0:\n",
    "                pbar.set_description(f\"Epoch {epoch+1}, Itr {idx+1}, Loss {running_loss:.5f}\")\n",
    "        loss_list.append(np.mean(epoch_loss))\n",
    "    return loss_list\n",
    "\n",
    "\n",
    "\n",
    "def testing(model):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    total_corr = 0\n",
    "    with torch.no_grad():\n",
    "        total_corr = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.type(torch.float32).to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            pred = torch.argmax(output, dim=1)\n",
    "            gt = torch.argmax(target, dim=1)\n",
    "            \n",
    "            correct = torch.count_nonzero(pred == gt)\n",
    "            total_corr += correct\n",
    "    accuracy = (total_corr/len(test_dataset))\n",
    "    return accuracy\n",
    "\n",
    "def num_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e06b38bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Itr 96, Loss 1.52377: 100%|████████████| 98/98 [00:08<00:00, 12.19it/s]\n",
      "Epoch 2, Itr 96, Loss 1.49272: 100%|████████████| 98/98 [00:07<00:00, 12.83it/s]\n",
      "Epoch 3, Itr 96, Loss 1.47240: 100%|████████████| 98/98 [00:07<00:00, 12.66it/s]\n",
      "Epoch 4, Itr 96, Loss 1.33846: 100%|████████████| 98/98 [00:07<00:00, 12.80it/s]\n",
      "Epoch 5, Itr 96, Loss 1.37078: 100%|████████████| 98/98 [00:07<00:00, 12.72it/s]\n",
      "Epoch 6, Itr 96, Loss 1.25819: 100%|████████████| 98/98 [00:07<00:00, 12.51it/s]\n",
      "Epoch 7, Itr 96, Loss 1.33170: 100%|████████████| 98/98 [00:07<00:00, 13.12it/s]\n",
      "Epoch 8, Itr 96, Loss 1.23138: 100%|████████████| 98/98 [00:07<00:00, 12.55it/s]\n",
      "Epoch 9, Itr 96, Loss 1.35579: 100%|████████████| 98/98 [00:07<00:00, 12.95it/s]\n",
      "Epoch 10, Itr 96, Loss 1.31721: 100%|███████████| 98/98 [00:07<00:00, 12.67it/s]\n",
      "Epoch 11, Itr 96, Loss 1.11329: 100%|███████████| 98/98 [00:07<00:00, 12.42it/s]\n",
      "Epoch 12, Itr 96, Loss 1.18969: 100%|███████████| 98/98 [00:07<00:00, 12.69it/s]\n",
      "Epoch 13, Itr 96, Loss 1.16186: 100%|███████████| 98/98 [00:07<00:00, 13.08it/s]\n",
      "Epoch 14, Itr 96, Loss 1.25620: 100%|███████████| 98/98 [00:07<00:00, 12.97it/s]\n",
      "Epoch 15, Itr 96, Loss 1.15406: 100%|███████████| 98/98 [00:07<00:00, 12.69it/s]\n",
      "Epoch 16, Itr 96, Loss 1.14309: 100%|███████████| 98/98 [00:07<00:00, 12.45it/s]\n",
      "Epoch 17, Itr 96, Loss 1.19446: 100%|███████████| 98/98 [00:07<00:00, 12.58it/s]\n",
      "Epoch 18, Itr 96, Loss 1.17529: 100%|███████████| 98/98 [00:07<00:00, 12.74it/s]\n",
      "Epoch 19, Itr 96, Loss 1.24990: 100%|███████████| 98/98 [00:07<00:00, 12.76it/s]\n",
      "Epoch 20, Itr 96, Loss 1.16900: 100%|███████████| 98/98 [00:07<00:00, 12.47it/s]\n",
      "Epoch 21, Itr 96, Loss 1.23869: 100%|███████████| 98/98 [00:07<00:00, 12.43it/s]\n",
      "Epoch 22, Itr 96, Loss 1.08932: 100%|███████████| 98/98 [00:07<00:00, 12.69it/s]\n",
      "Epoch 23, Itr 96, Loss 1.11806: 100%|███████████| 98/98 [00:07<00:00, 12.96it/s]\n",
      "Epoch 24, Itr 96, Loss 1.05806: 100%|███████████| 98/98 [00:07<00:00, 12.63it/s]\n",
      "Epoch 25, Itr 96, Loss 1.05890: 100%|███████████| 98/98 [00:07<00:00, 12.79it/s]\n",
      "Epoch 26, Itr 96, Loss 1.07800: 100%|███████████| 98/98 [00:08<00:00, 12.08it/s]\n",
      "Epoch 27, Itr 96, Loss 1.18060: 100%|███████████| 98/98 [00:07<00:00, 12.81it/s]\n",
      "Epoch 28, Itr 96, Loss 1.05307: 100%|███████████| 98/98 [00:07<00:00, 12.85it/s]\n",
      "Epoch 29, Itr 96, Loss 1.09060: 100%|███████████| 98/98 [00:07<00:00, 12.60it/s]\n",
      "Epoch 30, Itr 96, Loss 1.15163: 100%|███████████| 98/98 [00:07<00:00, 13.00it/s]\n",
      "Epoch 31, Itr 96, Loss 1.03851: 100%|███████████| 98/98 [00:08<00:00, 12.18it/s]\n",
      "Epoch 32, Itr 96, Loss 1.07937: 100%|███████████| 98/98 [00:07<00:00, 12.98it/s]\n",
      "Epoch 33, Itr 96, Loss 1.11276: 100%|███████████| 98/98 [00:07<00:00, 12.84it/s]\n",
      "Epoch 34, Itr 96, Loss 0.96382: 100%|███████████| 98/98 [00:07<00:00, 12.60it/s]\n",
      "Epoch 35, Itr 96, Loss 0.99578: 100%|███████████| 98/98 [00:07<00:00, 12.82it/s]\n",
      "Epoch 36, Itr 96, Loss 1.02612: 100%|███████████| 98/98 [00:08<00:00, 12.13it/s]\n",
      "Epoch 37, Itr 96, Loss 1.02424: 100%|███████████| 98/98 [00:07<00:00, 12.81it/s]\n",
      "Epoch 38, Itr 96, Loss 0.98268: 100%|███████████| 98/98 [00:07<00:00, 12.59it/s]\n",
      "Epoch 39, Itr 96, Loss 1.04299: 100%|███████████| 98/98 [00:07<00:00, 12.84it/s]\n",
      "Epoch 40, Itr 96, Loss 1.05669: 100%|███████████| 98/98 [00:07<00:00, 12.54it/s]\n",
      "Epoch 41, Itr 96, Loss 1.06529: 100%|███████████| 98/98 [00:07<00:00, 12.42it/s]\n",
      "Epoch 42, Itr 96, Loss 1.03761: 100%|███████████| 98/98 [00:07<00:00, 12.86it/s]\n",
      "Epoch 43, Itr 96, Loss 0.96263: 100%|███████████| 98/98 [00:07<00:00, 12.69it/s]\n",
      "Epoch 44, Itr 96, Loss 0.91746: 100%|███████████| 98/98 [00:07<00:00, 12.89it/s]\n",
      "Epoch 45, Itr 96, Loss 1.04695: 100%|███████████| 98/98 [00:07<00:00, 12.53it/s]\n",
      "Epoch 46, Itr 96, Loss 0.97563: 100%|███████████| 98/98 [00:07<00:00, 12.36it/s]\n",
      "Epoch 47, Itr 96, Loss 0.91526: 100%|███████████| 98/98 [00:07<00:00, 12.83it/s]\n",
      "Epoch 48, Itr 96, Loss 0.98096: 100%|███████████| 98/98 [00:07<00:00, 12.76it/s]\n",
      "Epoch 49, Itr 96, Loss 0.94171: 100%|███████████| 98/98 [00:07<00:00, 12.97it/s]\n",
      "Epoch 50, Itr 96, Loss 0.87916: 100%|███████████| 98/98 [00:07<00:00, 12.73it/s]\n",
      "Epoch 51, Itr 96, Loss 0.99518: 100%|███████████| 98/98 [00:07<00:00, 12.47it/s]\n",
      "Epoch 52, Itr 96, Loss 0.95068: 100%|███████████| 98/98 [00:07<00:00, 12.64it/s]\n",
      "Epoch 53, Itr 96, Loss 0.92652: 100%|███████████| 98/98 [00:07<00:00, 12.95it/s]\n",
      "Epoch 54, Itr 96, Loss 0.97868: 100%|███████████| 98/98 [00:07<00:00, 12.91it/s]\n",
      "Epoch 55, Itr 96, Loss 0.90125: 100%|███████████| 98/98 [00:07<00:00, 12.79it/s]\n",
      "Epoch 56, Itr 96, Loss 0.90522: 100%|███████████| 98/98 [00:07<00:00, 12.42it/s]\n",
      "Epoch 57, Itr 96, Loss 0.87068: 100%|███████████| 98/98 [00:07<00:00, 12.60it/s]\n",
      "Epoch 58, Itr 96, Loss 0.90521: 100%|███████████| 98/98 [00:07<00:00, 12.79it/s]\n",
      "Epoch 59, Itr 96, Loss 0.96692: 100%|███████████| 98/98 [00:07<00:00, 12.71it/s]\n",
      "Epoch 60, Itr 96, Loss 0.89111: 100%|███████████| 98/98 [00:07<00:00, 12.89it/s]\n",
      "Epoch 61, Itr 96, Loss 0.80183: 100%|███████████| 98/98 [00:07<00:00, 12.40it/s]\n",
      "Epoch 62, Itr 96, Loss 0.78605: 100%|███████████| 98/98 [00:07<00:00, 12.71it/s]\n",
      "Epoch 63, Itr 96, Loss 0.90261: 100%|███████████| 98/98 [00:07<00:00, 12.90it/s]\n",
      "Epoch 64, Itr 96, Loss 0.86337: 100%|███████████| 98/98 [00:07<00:00, 12.66it/s]\n",
      "Epoch 65, Itr 96, Loss 0.80352: 100%|███████████| 98/98 [00:07<00:00, 12.86it/s]\n",
      "Epoch 66, Itr 96, Loss 0.92554: 100%|███████████| 98/98 [00:08<00:00, 12.20it/s]\n",
      "Epoch 67, Itr 96, Loss 0.78900: 100%|███████████| 98/98 [00:07<00:00, 12.70it/s]\n",
      "Epoch 68, Itr 96, Loss 0.83973: 100%|███████████| 98/98 [00:07<00:00, 12.74it/s]\n",
      "Epoch 69, Itr 96, Loss 0.77034: 100%|███████████| 98/98 [00:07<00:00, 12.56it/s]\n",
      "Epoch 70, Itr 96, Loss 0.74673: 100%|███████████| 98/98 [00:07<00:00, 12.73it/s]\n",
      "Epoch 71, Itr 96, Loss 0.85423: 100%|███████████| 98/98 [00:08<00:00, 12.22it/s]\n",
      "Epoch 72, Itr 96, Loss 0.72126: 100%|███████████| 98/98 [00:07<00:00, 12.89it/s]\n",
      "Epoch 73, Itr 96, Loss 0.88421: 100%|███████████| 98/98 [00:07<00:00, 12.57it/s]\n",
      "Epoch 74, Itr 96, Loss 0.78147: 100%|███████████| 98/98 [00:07<00:00, 12.87it/s]\n",
      "Epoch 75, Itr 96, Loss 0.75673: 100%|███████████| 98/98 [00:07<00:00, 12.64it/s]\n",
      "Epoch 76, Itr 96, Loss 0.77729: 100%|███████████| 98/98 [00:07<00:00, 12.45it/s]\n",
      "Epoch 77, Itr 96, Loss 0.84379: 100%|███████████| 98/98 [00:07<00:00, 12.78it/s]\n",
      "Epoch 78, Itr 96, Loss 0.82622: 100%|███████████| 98/98 [00:07<00:00, 12.57it/s]\n",
      "Epoch 79, Itr 96, Loss 0.77530: 100%|███████████| 98/98 [00:07<00:00, 12.89it/s]\n",
      "Epoch 80, Itr 96, Loss 0.79554: 100%|███████████| 98/98 [00:07<00:00, 12.58it/s]\n",
      "Epoch 81, Itr 96, Loss 0.79599: 100%|███████████| 98/98 [00:07<00:00, 12.43it/s]\n",
      "Epoch 82, Itr 96, Loss 0.70924: 100%|███████████| 98/98 [00:07<00:00, 12.87it/s]\n",
      "Epoch 83, Itr 96, Loss 0.71749: 100%|███████████| 98/98 [00:07<00:00, 12.64it/s]\n",
      "Epoch 84, Itr 96, Loss 0.76392: 100%|███████████| 98/98 [00:07<00:00, 12.76it/s]\n",
      "Epoch 85, Itr 96, Loss 0.87366: 100%|███████████| 98/98 [00:07<00:00, 12.63it/s]\n",
      "Epoch 86, Itr 96, Loss 0.78978: 100%|███████████| 98/98 [00:07<00:00, 12.37it/s]\n",
      "Epoch 87, Itr 96, Loss 0.76594: 100%|███████████| 98/98 [00:07<00:00, 12.65it/s]\n",
      "Epoch 88, Itr 96, Loss 0.75825: 100%|███████████| 98/98 [00:07<00:00, 12.90it/s]\n",
      "Epoch 89, Itr 96, Loss 0.77378: 100%|███████████| 98/98 [00:07<00:00, 12.54it/s]\n",
      "Epoch 90, Itr 96, Loss 0.66867: 100%|███████████| 98/98 [00:07<00:00, 12.86it/s]\n",
      "Epoch 91, Itr 96, Loss 0.72130: 100%|███████████| 98/98 [00:07<00:00, 12.37it/s]\n",
      "Epoch 92, Itr 96, Loss 0.74253: 100%|███████████| 98/98 [00:07<00:00, 12.49it/s]\n",
      "Epoch 93, Itr 96, Loss 0.73462: 100%|███████████| 98/98 [00:07<00:00, 12.82it/s]\n",
      "Epoch 94, Itr 96, Loss 0.71521: 100%|███████████| 98/98 [00:07<00:00, 12.53it/s]\n",
      "Epoch 95, Itr 96, Loss 0.74931: 100%|███████████| 98/98 [00:07<00:00, 12.83it/s]\n",
      "Epoch 96, Itr 96, Loss 0.73576: 100%|███████████| 98/98 [00:08<00:00, 12.08it/s]\n",
      "Epoch 97, Itr 96, Loss 0.73777: 100%|███████████| 98/98 [00:07<00:00, 12.77it/s]\n",
      "Epoch 98, Itr 96, Loss 0.67050: 100%|███████████| 98/98 [00:07<00:00, 12.82it/s]\n",
      "Epoch 99, Itr 96, Loss 0.73550: 100%|███████████| 98/98 [00:07<00:00, 12.58it/s]\n",
      "Epoch 100, Itr 96, Loss 0.64901: 100%|██████████| 98/98 [00:07<00:00, 12.81it/s]\n",
      "Epoch 1, Itr 96, Loss 2.31751: 100%|████████████| 98/98 [00:08<00:00, 12.01it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, Itr 96, Loss 2.31712: 100%|████████████| 98/98 [00:07<00:00, 12.82it/s]\n",
      "Epoch 3, Itr 96, Loss 2.30749: 100%|████████████| 98/98 [00:07<00:00, 12.95it/s]\n",
      "Epoch 4, Itr 96, Loss 2.32316: 100%|████████████| 98/98 [00:07<00:00, 12.54it/s]\n",
      "Epoch 5, Itr 96, Loss 2.32135: 100%|████████████| 98/98 [00:07<00:00, 12.68it/s]\n",
      "Epoch 6, Itr 96, Loss 2.32155: 100%|████████████| 98/98 [00:08<00:00, 12.22it/s]\n",
      "Epoch 7, Itr 96, Loss 2.31396: 100%|████████████| 98/98 [00:07<00:00, 12.75it/s]\n",
      "Epoch 8, Itr 96, Loss 2.32945: 100%|████████████| 98/98 [00:07<00:00, 12.62it/s]\n",
      "Epoch 9, Itr 96, Loss 2.30770: 100%|████████████| 98/98 [00:07<00:00, 12.79it/s]\n",
      "Epoch 10, Itr 96, Loss 2.32371: 100%|███████████| 98/98 [00:07<00:00, 12.28it/s]\n",
      "Epoch 11, Itr 96, Loss 2.31928: 100%|███████████| 98/98 [00:08<00:00, 12.07it/s]\n",
      "Epoch 12, Itr 96, Loss 2.31358: 100%|███████████| 98/98 [00:07<00:00, 12.65it/s]\n",
      "Epoch 13, Itr 96, Loss 2.31906: 100%|███████████| 98/98 [00:07<00:00, 12.60it/s]\n",
      "Epoch 14, Itr 96, Loss 2.32047: 100%|███████████| 98/98 [00:07<00:00, 12.88it/s]\n",
      "Epoch 15, Itr 96, Loss 2.31906: 100%|███████████| 98/98 [00:07<00:00, 12.68it/s]\n",
      "Epoch 16, Itr 96, Loss 2.32591: 100%|███████████| 98/98 [00:07<00:00, 12.50it/s]\n",
      "Epoch 17, Itr 96, Loss 2.31766: 100%|███████████| 98/98 [00:07<00:00, 12.88it/s]\n",
      "Epoch 18, Itr 96, Loss 2.33165: 100%|███████████| 98/98 [00:07<00:00, 12.54it/s]\n",
      "Epoch 19, Itr 96, Loss 2.32849: 100%|███████████| 98/98 [00:07<00:00, 12.74it/s]\n",
      "Epoch 20, Itr 96, Loss 2.31990: 100%|███████████| 98/98 [00:07<00:00, 12.54it/s]\n",
      "Epoch 21, Itr 96, Loss 2.30310: 100%|███████████| 98/98 [00:07<00:00, 12.35it/s]\n",
      "Epoch 22, Itr 96, Loss 2.31884: 100%|███████████| 98/98 [00:07<00:00, 12.46it/s]\n",
      "Epoch 23, Itr 96, Loss 2.31633: 100%|███████████| 98/98 [00:07<00:00, 12.66it/s]\n",
      "Epoch 24, Itr 96, Loss 2.29912: 100%|███████████| 98/98 [00:07<00:00, 12.65it/s]\n",
      "Epoch 25, Itr 96, Loss 2.32528: 100%|███████████| 98/98 [00:07<00:00, 12.52it/s]\n",
      "Epoch 26, Itr 96, Loss 2.31252: 100%|███████████| 98/98 [00:08<00:00, 12.11it/s]\n",
      "Epoch 27, Itr 96, Loss 2.32875: 100%|███████████| 98/98 [00:07<00:00, 12.67it/s]\n",
      "Epoch 28, Itr 96, Loss 2.31916: 100%|███████████| 98/98 [00:07<00:00, 12.69it/s]\n",
      "Epoch 29, Itr 96, Loss 2.31843: 100%|███████████| 98/98 [00:07<00:00, 12.47it/s]\n",
      "Epoch 30, Itr 96, Loss 2.31778: 100%|███████████| 98/98 [00:07<00:00, 12.78it/s]\n",
      "Epoch 31, Itr 96, Loss 2.31878: 100%|███████████| 98/98 [00:07<00:00, 12.36it/s]\n",
      "Epoch 32, Itr 96, Loss 2.31865: 100%|███████████| 98/98 [00:07<00:00, 12.65it/s]\n",
      "Epoch 33, Itr 96, Loss 2.31889: 100%|███████████| 98/98 [00:07<00:00, 12.82it/s]\n",
      "Epoch 34, Itr 96, Loss 2.33097: 100%|███████████| 98/98 [00:07<00:00, 12.54it/s]\n",
      "Epoch 35, Itr 96, Loss 2.31560: 100%|███████████| 98/98 [00:07<00:00, 12.76it/s]\n",
      "Epoch 36, Itr 96, Loss 2.32467: 100%|███████████| 98/98 [00:08<00:00, 12.14it/s]\n",
      "Epoch 37, Itr 96, Loss 2.32976: 100%|███████████| 98/98 [00:07<00:00, 12.89it/s]\n",
      "Epoch 38, Itr 96, Loss 2.33003: 100%|███████████| 98/98 [00:07<00:00, 12.77it/s]\n",
      "Epoch 39, Itr 96, Loss 2.32900: 100%|███████████| 98/98 [00:07<00:00, 12.63it/s]\n",
      "Epoch 40, Itr 96, Loss 2.31572: 100%|███████████| 98/98 [00:07<00:00, 12.86it/s]\n",
      "Epoch 41, Itr 96, Loss 2.33020: 100%|███████████| 98/98 [00:08<00:00, 12.09it/s]\n",
      "Epoch 42, Itr 96, Loss 2.32150: 100%|███████████| 98/98 [00:07<00:00, 12.83it/s]\n",
      "Epoch 43, Itr 96, Loss 2.32069: 100%|███████████| 98/98 [00:07<00:00, 12.59it/s]\n",
      "Epoch 44, Itr 96, Loss 2.32662: 100%|███████████| 98/98 [00:07<00:00, 12.90it/s]\n",
      "Epoch 45, Itr 96, Loss 2.32677: 100%|███████████| 98/98 [00:07<00:00, 12.55it/s]\n",
      "Epoch 46, Itr 96, Loss 2.32071: 100%|███████████| 98/98 [00:07<00:00, 12.35it/s]\n",
      "Epoch 47, Itr 96, Loss 2.31126: 100%|███████████| 98/98 [00:07<00:00, 12.83it/s]\n",
      "Epoch 48, Itr 96, Loss 2.31987: 100%|███████████| 98/98 [00:07<00:00, 12.54it/s]\n",
      "Epoch 49, Itr 96, Loss 2.32073: 100%|███████████| 98/98 [00:07<00:00, 12.80it/s]\n",
      "Epoch 50, Itr 96, Loss 2.31914: 100%|███████████| 98/98 [00:07<00:00, 12.52it/s]\n",
      "Epoch 51, Itr 96, Loss 2.31141: 100%|███████████| 98/98 [00:07<00:00, 12.48it/s]\n",
      "Epoch 52, Itr 96, Loss 2.32109: 100%|███████████| 98/98 [00:07<00:00, 13.01it/s]\n",
      "Epoch 53, Itr 96, Loss 2.32393: 100%|███████████| 98/98 [00:07<00:00, 12.60it/s]\n",
      "Epoch 54, Itr 96, Loss 2.32693: 100%|███████████| 98/98 [00:07<00:00, 12.83it/s]\n",
      "Epoch 55, Itr 96, Loss 2.32926: 100%|███████████| 98/98 [00:07<00:00, 12.59it/s]\n",
      "Epoch 56, Itr 96, Loss 2.32316: 100%|███████████| 98/98 [00:07<00:00, 12.27it/s]\n",
      "Epoch 57, Itr 96, Loss 2.32797: 100%|███████████| 98/98 [00:07<00:00, 12.57it/s]\n",
      "Epoch 58, Itr 96, Loss 2.32418: 100%|███████████| 98/98 [00:07<00:00, 12.73it/s]\n",
      "Epoch 59, Itr 96, Loss 2.31810: 100%|███████████| 98/98 [00:07<00:00, 12.51it/s]\n",
      "Epoch 60, Itr 96, Loss 2.31943: 100%|███████████| 98/98 [00:07<00:00, 12.79it/s]\n",
      "Epoch 61, Itr 96, Loss 2.31394: 100%|███████████| 98/98 [00:07<00:00, 12.48it/s]\n",
      "Epoch 62, Itr 96, Loss 2.32165: 100%|███████████| 98/98 [00:07<00:00, 12.63it/s]\n",
      "Epoch 63, Itr 96, Loss 2.32575: 100%|███████████| 98/98 [00:07<00:00, 12.83it/s]\n",
      "Epoch 64, Itr 96, Loss 2.31934: 100%|███████████| 98/98 [00:07<00:00, 12.54it/s]\n",
      "Epoch 65, Itr 96, Loss 2.32627: 100%|███████████| 98/98 [00:07<00:00, 12.81it/s]\n",
      "Epoch 66, Itr 96, Loss 2.31144: 100%|███████████| 98/98 [00:08<00:00, 12.13it/s]\n",
      "Epoch 67, Itr 96, Loss 2.31095: 100%|███████████| 98/98 [00:07<00:00, 12.75it/s]\n",
      "Epoch 68, Itr 96, Loss 2.31839: 100%|███████████| 98/98 [00:07<00:00, 12.72it/s]\n",
      "Epoch 69, Itr 96, Loss 2.31633: 100%|███████████| 98/98 [00:07<00:00, 12.56it/s]\n",
      "Epoch 70, Itr 96, Loss 2.31792: 100%|███████████| 98/98 [00:07<00:00, 12.77it/s]\n",
      "Epoch 71, Itr 96, Loss 2.32247: 100%|███████████| 98/98 [00:08<00:00, 12.14it/s]\n",
      "Epoch 72, Itr 96, Loss 2.31632: 100%|███████████| 98/98 [00:07<00:00, 12.80it/s]\n",
      "Epoch 73, Itr 96, Loss 2.32797: 100%|███████████| 98/98 [00:07<00:00, 12.63it/s]\n",
      "Epoch 74, Itr 96, Loss 2.31743: 100%|███████████| 98/98 [00:07<00:00, 12.75it/s]\n",
      "Epoch 75, Itr 96, Loss 2.31859: 100%|███████████| 98/98 [00:07<00:00, 12.84it/s]\n",
      "Epoch 76, Itr 96, Loss 2.32747: 100%|███████████| 98/98 [00:08<00:00, 12.13it/s]\n",
      "Epoch 77, Itr 96, Loss 2.31146: 100%|███████████| 98/98 [00:07<00:00, 12.90it/s]\n",
      "Epoch 78, Itr 96, Loss 2.31717: 100%|███████████| 98/98 [00:07<00:00, 12.57it/s]\n",
      "Epoch 79, Itr 96, Loss 2.32458: 100%|███████████| 98/98 [00:07<00:00, 12.61it/s]\n",
      "Epoch 80, Itr 96, Loss 2.32497: 100%|███████████| 98/98 [00:07<00:00, 12.44it/s]\n",
      "Epoch 81, Itr 96, Loss 2.31682: 100%|███████████| 98/98 [00:08<00:00, 12.23it/s]\n",
      "Epoch 82, Itr 96, Loss 2.31158: 100%|███████████| 98/98 [00:07<00:00, 12.72it/s]\n",
      "Epoch 83, Itr 96, Loss 2.32991: 100%|███████████| 98/98 [00:07<00:00, 12.49it/s]\n",
      "Epoch 84, Itr 96, Loss 2.32139: 100%|███████████| 98/98 [00:07<00:00, 12.57it/s]\n",
      "Epoch 85, Itr 96, Loss 2.30985: 100%|███████████| 98/98 [00:07<00:00, 12.44it/s]\n",
      "Epoch 86, Itr 96, Loss 2.32309: 100%|███████████| 98/98 [00:07<00:00, 12.34it/s]\n",
      "Epoch 87, Itr 96, Loss 2.32660: 100%|███████████| 98/98 [00:07<00:00, 12.46it/s]\n",
      "Epoch 88, Itr 96, Loss 2.32056: 100%|███████████| 98/98 [00:07<00:00, 12.88it/s]\n",
      "Epoch 89, Itr 96, Loss 2.32532: 100%|███████████| 98/98 [00:07<00:00, 12.73it/s]\n",
      "Epoch 90, Itr 96, Loss 2.31935: 100%|███████████| 98/98 [00:07<00:00, 12.48it/s]\n",
      "Epoch 91, Itr 96, Loss 2.30788: 100%|███████████| 98/98 [00:08<00:00, 12.20it/s]\n",
      "Epoch 92, Itr 96, Loss 2.31832: 100%|███████████| 98/98 [00:07<00:00, 12.42it/s]\n",
      "Epoch 93, Itr 96, Loss 2.31895: 100%|███████████| 98/98 [00:07<00:00, 12.90it/s]\n",
      "Epoch 94, Itr 96, Loss 2.30496: 100%|███████████| 98/98 [00:07<00:00, 12.51it/s]\n",
      "Epoch 95, Itr 96, Loss 2.32404: 100%|███████████| 98/98 [00:07<00:00, 12.82it/s]\n",
      "Epoch 96, Itr 96, Loss 2.31593: 100%|███████████| 98/98 [00:07<00:00, 12.29it/s]\n",
      "Epoch 97, Itr 96, Loss 2.31774: 100%|███████████| 98/98 [00:07<00:00, 12.53it/s]\n",
      "Epoch 98, Itr 96, Loss 2.31503: 100%|███████████| 98/98 [00:07<00:00, 12.70it/s]\n",
      "Epoch 99, Itr 96, Loss 2.32511: 100%|███████████| 98/98 [00:07<00:00, 12.46it/s]\n",
      "Epoch 100, Itr 96, Loss 2.30980: 100%|██████████| 98/98 [00:07<00:00, 12.78it/s]\n"
     ]
    }
   ],
   "source": [
    "loss_without_dropout = training(model=model_without_dropout)\n",
    "loss_with_dropout = training(model=model_with_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "98deaa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Acc of the model with dropout is: 9.999999046325684\n",
      "The Acc of the model without dropout is: 44.70000076293945\n"
     ]
    }
   ],
   "source": [
    "print(f\"The Acc of the model with dropout is: {testing(model_with_dropout) * 100}\")\n",
    "print(f\"The Acc of the model without dropout is: {testing(model_without_dropout) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "656c4764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params with dropout 62006\n",
      "Number of params without dropout 62006\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of params with dropout {num_params(model_with_dropout)}\")\n",
    "print(f\"Number of params without dropout {num_params(model_without_dropout)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe415d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
