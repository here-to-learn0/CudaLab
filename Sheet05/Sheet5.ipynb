{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sheet5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms"
      ],
      "metadata": {
        "id": "HfIlsf1m-W-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading and Loading Dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(),download=True)\n",
        " \n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())"
      ],
      "metadata": {
        "id": "UvUJWNh5-oL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting data loaders for iterating\n",
        "B_SIZE = 256\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=B_SIZE, \n",
        "                                           shuffle=True) \n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=B_SIZE,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "3zDDe_Hf-yt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Cells(torch.nn.Module):\n",
        "   \n",
        "    def __init__(self, input_dim,  hidden_dim):\n",
        "        super(LSTM_Cells, self).__init__()\n",
        "        self.hidden_dim =  hidden_dim\n",
        "        self.input_dim =  input_dim\n",
        "\n",
        "        # forget gate components\n",
        "        self.input_gate1 = nn.Linear(self.input_dim, self.hidden_dim, bias=True)\n",
        "        self.input_gate2 = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
        "\n",
        "        # forget gate layers\n",
        "        self.forget1 = nn.Linear(self.input_dim, self.hidden_dim, bias=True)\n",
        "        self.forget2 = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
        "\n",
        "        # cell layers\n",
        "        self.cell_1 = nn.Linear(self.input_dim, self.hidden_dim, bias=True)\n",
        "        self.cell_2 = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
        "        \n",
        "\n",
        "        # output gate layers\n",
        "        self.out_gate1 = nn.Linear(self.input_dim, self.hidden_dim, bias=True)\n",
        "        self.out_gate2 = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanH = nn.Tanh()\n",
        "\n",
        "    def cell_gate(self,  x, h ,input_gate_output, forget_gate_output, cell_prev):\n",
        "        x =  self.cell_1(x)\n",
        "        h =  self.cell_2(h)\n",
        "        cur_info = self.tanH(x + h)\n",
        "        new_info = cur_info * input_gate_output\n",
        "        forget_info = forget_gate_output * cell_prev\n",
        "        c_next = new_info + forget_info\n",
        "        return c_next\n",
        "      \n",
        "    def other_gates(self, x, h, gate1, gate2):\n",
        "        x = gate1(x)\n",
        "        h = gate2(h)\n",
        "        return self.sigmoid(x + h)\n",
        "    \n",
        "    def forward(self, embeddings, h, c):\n",
        "        \n",
        "        input_gate_output = self.other_gates(embeddings, h,self.input_gate1, self.input_gate2)\n",
        "       \n",
        "        forget_gate_output = self.other_gates(embeddings, h,self.forget1, self.forget2)        \n",
        "       \n",
        "        c_next = self.cell_gate(embeddings, h, input_gate_output, forget_gate_output, c)\n",
        "        \n",
        "        o = self.other_gates(embeddings, h, self.out_gate1,  self.out_gate2)\n",
        "       \n",
        "        h_next = o * self.tanH(c_next)\n",
        "        return c_next, h_next \n"
      ],
      "metadata": {
        "id": "G3F_oG5zr1lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRKnWBXS-PPb"
      },
      "outputs": [],
      "source": [
        "class SequentialClassifierWithCells(nn.Module):\n",
        "    \"\"\" \n",
        "    Sequential classifier for images. Embedded image rows are fed to a RNN\n",
        "    \n",
        "    Args:\n",
        "    -----\n",
        "    input_dim: integer\n",
        "        dimensionality of the rows to embed\n",
        "    emb_dim: integer \n",
        "        dimensionality of the vectors fed to the LSTM\n",
        "    hidden_dim: integer\n",
        "        dimensionality of the states in the cell\n",
        "    num_layers: integer\n",
        "        number of stacked LSTMS\n",
        "    mode: string\n",
        "        intialization of the states\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers=1, mode=\"zeros\"):\n",
        "        \"\"\" Module initializer \"\"\"\n",
        "        assert mode in [\"zeros\", \"random\", \"learned\"]\n",
        "        super().__init__()\n",
        "        self.hidden_dim =  hidden_dim\n",
        "        self.input_dim =  input_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.mode = mode\n",
        "        if(mode == \"learned\"):\n",
        "            self.learned_h = nn.Parameter(\n",
        "                    torch.randn(num_layers, 1, hidden_dim).requires_grad_()\n",
        "                )\n",
        "            self.learned_c = nn.Parameter(\n",
        "                    torch.randn(num_layers, 1, hidden_dim).requires_grad_()\n",
        "                )\n",
        "        \n",
        "        # for embedding rows into vector representations\n",
        "        self.encoder = nn.Linear(in_features=input_dim, out_features=emb_dim)\n",
        "\n",
        "        lstms = []\n",
        "        for i in range(num_layers):\n",
        "            in_size = emb_dim if i == 0 else hidden_dim\n",
        "            lstms.append(LSTM_Cells(input_dim=input_dim, hidden_dim=hidden_dim))\n",
        "        self.lstm = nn.ModuleList(lstms)\n",
        "        \n",
        "        # classifier\n",
        "        self.classifier = nn.Linear(in_features=hidden_dim, out_features=10)\n",
        "        \n",
        "        return\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Forward pass through model \"\"\"\n",
        "        \n",
        "        b_size, n_channels, n_rows, n_cols = x.shape\n",
        "        h, c = self.init_state(b_size=b_size, device=x.device) \n",
        "        \n",
        "        # embedding rows\n",
        "        x_rowed = x.view(b_size, n_channels*n_rows, n_cols)\n",
        "\n",
        "        embeddings = self.encoder(x_rowed)\n",
        "\n",
        "\n",
        "        lstm_out = []\n",
        "        for i in range(embeddings.shape[1]):\n",
        "            lstm_input = embeddings[:, i, :]\n",
        "            # iterating over LSTM Cells\n",
        "            for j, lstm_cell in enumerate(self.lstm):\n",
        "                h[j], c[j] = lstm_cell(lstm_input, h[j], c[j])\n",
        "                lstm_input = h[j]\n",
        "            lstm_out.append(lstm_input)\n",
        "        lstm_out = torch.stack(lstm_out, dim=1)\n",
        "\n",
        "\n",
        "        \n",
        "        # classifying\n",
        "        y = self.classifier(lstm_out[:, -1, :])  # feeding only output at last layer\n",
        "        \n",
        "        return y\n",
        "    \n",
        "        \n",
        "    def init_state(self, b_size, device):\n",
        "        \"\"\" Initializing hidden and cell state \"\"\"\n",
        "        if(self.mode == \"zeros\"):\n",
        "            h = torch.zeros(self.num_layers, b_size, self.hidden_dim)\n",
        "            c = torch.zeros(self.num_layers, b_size, self.hidden_dim)\n",
        "        elif(self.mode == \"random\"):\n",
        "            h = torch.randn(self.num_layers, b_size, self.hidden_dim)\n",
        "            c = torch.randn(self.num_layers, b_size, self.hidden_dim)\n",
        "        elif(self.mode == \"learned\"):\n",
        "            h = self.learned_h.repeat(1, b_size, 1)\n",
        "            c = self.learned_c.repeat(1, b_size, 1)\n",
        "        h = h.to(device)\n",
        "        c = c.to(device)\n",
        "        return h, c"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_loader, optimizer, criterion, epoch, device):\n",
        "    \"\"\" Training a model for one epoch \"\"\"\n",
        "    \n",
        "    loss_list = []\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for i, (images, labels) in progress_bar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "         \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "         \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss_list.append(loss.item())\n",
        "         \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "         \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        progress_bar.set_description(f\"Epoch {epoch+1} Iter {i+1}: loss {loss.item():.5f}. \")\n",
        "        \n",
        "    mean_loss = np.mean(loss_list)\n",
        "    return mean_loss, loss_list\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model(model, eval_loader, criterion, device):\n",
        "    \"\"\" Evaluating the model for either validation or test \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_list = []\n",
        "    \n",
        "    for images, labels in eval_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass only to get logits/output\n",
        "        outputs = model(images)\n",
        "                 \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss_list.append(loss.item())\n",
        "            \n",
        "        # Get predictions from the maximum value\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct += len( torch.where(preds==labels)[0] )\n",
        "        total += len(labels)\n",
        "                 \n",
        "    # Total correct predictions and loss\n",
        "    accuracy = correct / total * 100\n",
        "    loss = np.mean(loss_list)\n",
        "    \n",
        "    return accuracy, loss\n",
        "\n",
        "\n",
        "def train_model(model, optimizer, scheduler, criterion, train_loader, valid_loader, num_epochs):\n",
        "    \"\"\" Training a model for a given number of epochs\"\"\"\n",
        "    \n",
        "    train_loss = []\n",
        "    val_loss =  []\n",
        "    loss_iters = []\n",
        "    valid_acc = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "           \n",
        "        # validation epoch\n",
        "        model.eval()  # important for dropout and batch norms\n",
        "        accuracy, loss = eval_model(\n",
        "                    model=model, eval_loader=valid_loader,\n",
        "                    criterion=criterion, device=device\n",
        "            )\n",
        "        valid_acc.append(accuracy)\n",
        "        val_loss.append(loss)\n",
        "        \n",
        "        # training epoch\n",
        "        model.train()  # important for dropout and batch norms\n",
        "        mean_loss, cur_loss_iters = train_epoch(\n",
        "                model=model, train_loader=train_loader, optimizer=optimizer,\n",
        "                criterion=criterion, epoch=epoch, device=device\n",
        "            )\n",
        "        scheduler.step()\n",
        "        train_loss.append(mean_loss)\n",
        "        loss_iters = loss_iters + cur_loss_iters\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"    Train loss: {round(mean_loss, 5)}\")\n",
        "        print(f\"    Valid loss: {round(loss, 5)}\")\n",
        "        print(f\"    Accuracy: {accuracy}%\")\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(f\"Training completed\")\n",
        "    return train_loss, val_loss, loss_iters, valid_acc\n",
        "\n",
        "\n",
        "def smooth(f, K=5):\n",
        "    \"\"\" Smoothing a function using a low-pass filter (mean) of size K \"\"\"\n",
        "    kernel = np.ones(K) / K\n",
        "    f = np.concatenate([f[:int(K//2)], f, f[int(-K//2):]])  # to account for boundaries\n",
        "    smooth_f = np.convolve(f, kernel, mode=\"same\")\n",
        "    smooth_f = smooth_f[K//2: -K//2]  # removing boundary-fixes\n",
        "    return smooth_f\n",
        "\n",
        "def count_model_params(model):\n",
        "    \"\"\" Counting the number of learnable parameters in a nn.Module \"\"\"\n",
        "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return num_params"
      ],
      "metadata": {
        "id": "0HFlPsDB-TGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "0LFtN34znIwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SequentialClassifierWithCells(input_dim=28, emb_dim=64, hidden_dim=128, num_layers=2, mode=\"zeros\")\n",
        "count_model_params(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAgjXYfZnekC",
        "outputId": "02de2f34-ba16-4f8f-be52-d1bac6e6f49f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "163914"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "Cv7AShZ6npUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classification loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 5 epochs\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.2)"
      ],
      "metadata": {
        "id": "xa1WGinznv-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, val_loss, loss_iters, valid_acc = train_model(\n",
        "        model=model, optimizer=optimizer, scheduler=scheduler, criterion=criterion,\n",
        "        train_loader=train_loader, valid_loader=test_loader, num_epochs=10\n",
        "    )"
      ],
      "metadata": {
        "id": "8-Yeg-r6M3lK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}